name: Polymarket (every 6h) - crawl + stats (UTC)

on:
  workflow_dispatch:
  schedule:
    - cron: "0 */6 * * *"  # every 6 hours (UTC): 00:00, 06:00, 12:00, 18:00

concurrency:
  group: polymarket-6h-${{ github.ref }}
  cancel-in-progress: false

permissions:
  contents: read

jobs:
  run:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests

      - name: Run crawl then stats (single run)
        env:
          # Fine-grained PAT (repo contents read/write for these repos)
          POLYMARKET_GH_PAT: ${{ secrets.POLYMARKET_GH_PAT }}

          # Org/user who owns repos
          ORG: statground

          # Target repo name prefix
          REPO_PREFIX: Statground_Data_Polymarket

          # API base
          POLYMARKET_API_BASE: https://gamma-api.polymarket.com

          # Fetch settings (tune as needed)
          PAGE_LIMIT: "100"
          MAX_PAGES: "200"
          ORDER_PRIMARY: updatedAt
          ORDER_FALLBACK: id

          # Folder root inside each repo
          OUT_ROOT: by_created

          # Where checkpoint/targets stored in orchestrator repo
          CHECKPOINT_PATH: .state/polymarket_checkpoint.json
          TARGET_REPOS_PATH: .state/polymarket_targets.json

          # Fan-out counts file (written into each year repo)
          COUNTS_FILE_NAME: POLYMARKET_COUNTS.json

          # Stats output (written into orchestrator repo)
          STATS_MD_PATH: POLYMARKET_REPO_STATS.md
        run: |
          python scripts/polymarket_hourly_scheduler.py
